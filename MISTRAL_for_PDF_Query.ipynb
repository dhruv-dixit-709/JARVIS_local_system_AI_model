{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_community.llms import LlamaCpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader = PyPDFDirectoryLoader(\"/data_files/\")\n",
    "loader = PyPDFDirectoryLoader(\"D:\\\\Projects\\\\All Purpose Chatbot\\\\Mistral_Implementation\\\\data_files\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 20)\n",
    "\n",
    "text_chunks = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(text_chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder fetch and generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.from_documents(text_chunks, \n",
    "                                    embedding = embeddings\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LlamaCpp(\n",
    "    streaming = True,\n",
    "    model_path = \"D:\\\\Projects\\\\All Purpose Chatbot\\\\Mistral_Implementation\\\\mistral-7b-instruct-v0.1.Q2_K.gguf\",\n",
    "    temperature = 0.75,\n",
    "    top_p = 1,\n",
    "    verbose = True,\n",
    "    n_ctx = 4096\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_template = PromptTemplate(\n",
    "#     input_variables=[\"context\", \"question\"],\n",
    "#     template=(\n",
    "#         \"You are a knowledgeable assistant. Answer the question based on the following context.\\n\\n\"\n",
    "#         \"Context:\\n{context}\\n\\n\"\n",
    "#         \"Question: {question}\\n\\n\"\n",
    "#         \"If the answer is not in the context, say 'Sorry, I don't have answer for that.'.\"\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm = llm, \n",
    "                                 chain_type = \"stuff\", \n",
    "                                 retriever = vector_store.as_retriever(search_kwargs = {\"k\": 2})\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qa = RetrievalQA.from_chain_type(llm = llm, \n",
    "#                                  chain_type = \"stuff\", \n",
    "#                                  retriever = vector_store.as_retriever(search_kwargs = {\"k\": 2}),\n",
    "#                                  chain_type_kwargs = {\"prompt\": prompt_template}\n",
    "#                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who made Saturn V?\"\n",
    "qa.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"I want to know about buying any property?\"\n",
    "qa.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who is Megha?\"\n",
    "qa.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who are Rahul and Dhruv?\"\n",
    "qa.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How can we buy real estate property from Dhruv?\"\n",
    "qa.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How to rent a room?\"\n",
    "qa.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question2 = \"What was the name of the station that Saturn V launched and how many stages did it have?\"\n",
    "qa.run(question2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question2 = \"Who is Sachin Tendulkar?\"\n",
    "qa.run(question2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question2 = \"Who is Sachin Tendulkar?\"\n",
    "# qa.run(question2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while True:\n",
    "#   user_input = input(f\"Input Prompt: \")\n",
    "#   if user_input == 'exit':\n",
    "#     print('Exiting')\n",
    "#     sys.exit()\n",
    "#   if user_input == '':\n",
    "#     continue\n",
    "#   result = qa({'query': user_input})\n",
    "#   print(f\"Answer: {result['result']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Automatic for loop for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to log intermediate outputs\n",
    "# def log_intermediate_steps(qa_chain, question):\n",
    "#     retriever = qa_chain.retriever\n",
    "#     docs = retriever.get_relevant_documents(question)\n",
    "#     context = \"\\n\".join([doc.page_content for doc in docs])\n",
    "#     prompt = qa_chain.llm_chain.prompt.format(context=context, question=question)\n",
    "    \n",
    "#     print(\"Retrieved Documents:\\n\", context)\n",
    "#     print(\"Generated Prompt:\\n\", prompt)\n",
    "    \n",
    "#     return context, prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_intermediate_steps(retriever, question):\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    context = \"\\n\".join([doc.page_content for doc in docs])\n",
    "    prompt = prompt_template.format(context=context, question=question)\n",
    "    \n",
    "    print(\"Retrieved Documents:\\n\", context)\n",
    "    print(\"Generated Prompt:\\n\", prompt)\n",
    "    \n",
    "    return context, prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who made Saturn V?\"\n",
    "context, prompt = log_intermediate_steps(qa, question)\n",
    "answer = qa.run(question)\n",
    "\n",
    "print(\"Answer:\\n\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
