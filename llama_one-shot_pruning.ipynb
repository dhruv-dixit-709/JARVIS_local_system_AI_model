{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from llama import Llama\n",
    "from short_llama import ShortLlama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"pg19\", \n",
    "                    split = \"validation\"\n",
    "                    )  # authors sample 10,000 texts to compute block influences\n",
    "dataloader = DataLoader(\n",
    "    data,\n",
    "    batch_size = 1,\n",
    "    shuffle = True,\n",
    "    generator = torch.Generator(device = \"cuda\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch and Wrap Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 1024  # authors use a context width of 1024\n",
    "llama = Llama.build(\n",
    "    ckpt_dir = \"../../llama/llama-2-7b\",\n",
    "    tokenizer_path = \"../../llama/tokenizer.model\",\n",
    "    max_seq_len = MAX_SEQ_LEN,\n",
    "    max_batch_size = 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_llama = ShortLlama(llama = llama, \n",
    "                         n_prune_layers = 9\n",
    "                         )\n",
    "\n",
    "short_llama.llama.model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample generation\n",
    "short_llama.llama.text_completion(\n",
    "    prompts = [\"I am an avid fan of \"],\n",
    "    max_gen_len = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in tqdm(dataloader):\n",
    "    prompts = batch['text']\n",
    "\n",
    "    prompt_tokens = [short_llama.llama.tokenizer.encode(x, bos = True, eos = False) for x in prompts]\n",
    "    max_prompt_len = max(len(t) for t in prompt_tokens)\n",
    "\n",
    "    # authors use a sliding window of size 1024 with a shift of 256\n",
    "    for start in range(0, max_prompt_len, 256):\n",
    "\n",
    "        inputs = [p[start:start+MAX_SEQ_LEN] for p in prompt_tokens if len(p) > start]\n",
    "\n",
    "        short_llama.eval_importance(\n",
    "            prompt_tokens = inputs,\n",
    "            max_gen_len = 0\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_llama.importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove unimportant layers\n",
    "\n",
    "Layers removed when using pg19 val set: [25, 27, 24, 26, 28, 29, 23, 22, 21]\n",
    "\n",
    "Note: Different order than paper but same 9 least important layers -> [27, 26, 25, 28, 24, 29, 23, 21, 22]\n",
    "\n",
    "Additionally, authors mention that the layer order is quite nuanced and can vary with different datasets. However, relative order suggests similar importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_llama.remove_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_llama.llama.model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the paper states: \\\n",
    "    - \"Our experiments reveal that the effect of layer removal is significantly more pronounced on generative\n",
    "        tasks compared to multiple-choice tasks. On benchmarks such as GSM8K (Cobbe et al., 2021) and\n",
    "        HumanEval (Chen et al., 2021), removing 25% of the layers often leads to a severe performance\n",
    "        drop, with scores approaching zero.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_llama.llama.text_completion(\n",
    "    prompts = [\"I am an avid fan of \"],\n",
    "    max_gen_len = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Angular Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in tqdm(dataloader):\n",
    "    prompts = batch['text']\n",
    "\n",
    "    prompt_tokens = [short_llama.llama.tokenizer.encode(x, bos = True, eos = False) for x in prompts]\n",
    "    max_prompt_len = max(len(t) for t in prompt_tokens)\n",
    "\n",
    "    # authors use a sliding window of size 1024 with a shift of 256\n",
    "    for start in range(0, max_prompt_len, 256):\n",
    "\n",
    "        inputs = [p[start:start+MAX_SEQ_LEN] for p in prompt_tokens if len(p) > start]\n",
    "\n",
    "        short_llama.eval_importance(\n",
    "            prompt_tokens = inputs,\n",
    "            max_gen_len = 0,\n",
    "            angular = True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_llama.importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove unimportant layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_llama.remove_layers(angular = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_llama.llama.model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_llama.llama.text_completion(\n",
    "    prompts = [\"I am an avid fan of \"],\n",
    "    max_gen_len = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shortgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
